# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dV0QZMI7kCvxu8dp4Suz3xl6Kc7gbYf0
"""

import kagglehub
import zipfile
import os
import pandas as pd

# Step 1: Download dataset
path = kagglehub.dataset_download("chiragksharma/transformed-housing-data")
print("Downloaded to:", path)

# Step 2: Extract if it's a zip
if path.endswith(".zip"):
    with zipfile.ZipFile(path, 'r') as zip_ref:
        zip_ref.extractall("housing_data")  # Extract to folder
    print("Files extracted to 'housing_data' folder")
    folder_path = "housing_data"
else:
    folder_path = path

# Step 3: Check files
print("Files in dataset folder:", os.listdir(folder_path))

# Step 4: Load CSV (replace with actual CSV name from folder)
csv_file = os.path.join(folder_path, "Transformed_Housing_Data2.csv")
data = pd.read_csv(csv_file)

# Step 5: Preview
print(data.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
import kagglehub
import zipfile
import os

warnings.filterwarnings("ignore")

# ---------------------------
# Step 1: Download dataset
# ---------------------------
dataset_path = kagglehub.dataset_download("chiragksharma/transformed-housing-data")
print("Downloaded dataset to:", dataset_path)

# Step 2: Determine data folder and extract if necessary
data_folder = ""
if os.path.isdir(dataset_path):
    data_folder = dataset_path
    print(f"Dataset is a directory, using '{data_folder}' directly.")
elif dataset_path.endswith(".zip"):
    extract_folder = "housing_data"
    os.makedirs(extract_folder, exist_ok=True) # Ensure the extraction folder exists
    with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
        zip_ref.extractall(extract_folder)
    print(f"Extracted files to folder: '{extract_folder}'")
    data_folder = extract_folder
else:
    raise ValueError(f"Unsupported dataset format: {dataset_path}. Expected a directory or a .zip file.")

# ---------------------------
# Step 3: Automatically find CSV file
# ---------------------------
csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]
if not csv_files:
    raise FileNotFoundError(f"No CSV file found in the data folder: {data_folder}!")
csv_file_path = os.path.join(data_folder, csv_files[0])
print("Using CSV file:", csv_file_path)

# ---------------------------
# Step 4: Load dataset
# ---------------------------
data = pd.read_csv(csv_file_path)
print("Raw Dataset:")
display(data.head())

# ---------------------------
# Step 5: Feature Scaling & VIF calculation
# ---------------------------
Y = data['Sale_Price']
X = data.drop(columns=['Sale_Price'])

# First, scale all features for VIF calculation
scaler_for_vif_calc = StandardScaler()
X_scaled_for_vif_calc = scaler_for_vif_calc.fit_transform(X)
X_scaled_for_vif_calc = pd.DataFrame(X_scaled_for_vif_calc, columns=X.columns)

print("\nScaled Features (for VIF calculation):")
display(X_scaled_for_vif_calc.head())

def calculate_vif(df):
    vif = pd.Series(
        [variance_inflation_factor(df.values, i) for i in range(df.shape[1])],
        index=df.columns
    )
    return vif

# Identify high VIF features from the fully scaled data
vif_scores = calculate_vif(X_scaled_for_vif_calc)
high_vif_features = vif_scores[vif_scores > 5].index.tolist()

# Drop high VIF features from the original unscaled X
X_filtered_unscaled = X.drop(columns=high_vif_features)

# Now, fit a new scaler on the filtered unscaled features for the model
scaler_for_model = StandardScaler()
X_processed_for_model = scaler_for_model.fit_transform(X_filtered_unscaled)
X_processed_for_model = pd.DataFrame(X_processed_for_model, columns=X_filtered_unscaled.columns)

# Assign to X_vif for consistency with subsequent steps
X_vif = X_processed_for_model

print("\nRemaining Features after VIF check (unscaled, then scaled for model):")
display(X_vif.head())
print("\nVIF values for remaining features (calculated on initial scaled set):")
# Recalculate VIF on the final processed features for display
display(calculate_vif(X_vif))

# ---------------------------
# Step 7: Train-Test Split
# ---------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_vif, Y, test_size=0.3, random_state=101
)

# ---------------------------
# Step 8: Linear Regression
# ---------------------------
model = LinearRegression()
model.fit(X_train, y_train)

r2_score = model.score(X_test, y_test)
print(f"\nR² Score on Test Set: {r2_score:.4f}")

# ---------------------------
# Step 9: Predictions & Residuals
# ---------------------------
predictions = model.predict(X_test)
residuals = predictions - y_test

# Residual Plot
plt.figure(figsize=(8,5))
plt.scatter(predictions, residuals, s=10)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted Sale Price")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import LinearRegression
# from sklearn.model_selection import train_test_split
# from statsmodels.stats.outliers_influence import variance_inflation_factor
# 
# # Title
# st.title("Housing Price Prediction")
# 
# # Upload CSV
# uploaded_file = st.file_uploader("Upload your housing dataset CSV", type="csv")
# if uploaded_file:
#     data = pd.read_csv(uploaded_file)
#     st.subheader("Raw Data")
#     st.dataframe(data.head())
# 
#     # Features & Target
#     Y = data['Sale_Price']
#     X = data.drop(columns=['Sale_Price'])
# 
#     # Function to calculate VIF
#     def calculate_vif(df):
#         return pd.Series(
#             [variance_inflation_factor(df.values, i) for i in range(df.shape[1])],
#             index=df.columns
#         )
# 
#     # Step 1: Scale all features for VIF calculation
#     scaler_for_vif_calc = StandardScaler()
#     X_scaled_for_vif_calc = scaler_for_vif_calc.fit_transform(X)
#     X_scaled_for_vif_calc = pd.DataFrame(X_scaled_for_vif_calc, columns=X.columns)
# 
#     # Step 2: Identify high VIF features
#     vif_scores = calculate_vif(X_scaled_for_vif_calc)
#     high_vif_features = vif_scores[vif_scores > 5].index.tolist()
# 
#     # Step 3: Drop high VIF features from the original unscaled X
#     X_filtered_unscaled = X.drop(columns=high_vif_features)
# 
#     # Step 4: Fit a new scaler on the filtered unscaled features for the model
#     scaler_for_model = StandardScaler()
#     X_processed_for_model = scaler_for_model.fit_transform(X_filtered_unscaled)
#     X_processed_for_model = pd.DataFrame(X_processed_for_model, columns=X_filtered_unscaled.columns)
# 
#     st.subheader("Features after VIF check and Scaling for Model")
#     st.dataframe(X_processed_for_model.head())
# 
#     # Assign to X_vif for consistency with previous notebook variable naming
#     X_vif = X_processed_for_model
# 
#     # Train-test split
#     X_train, X_test, y_train, y_test = train_test_split(X_vif, Y, test_size=0.3, random_state=101)
# 
#     # Model
#     model = LinearRegression()
#     model.fit(X_train, y_train)
# 
#     st.subheader("Model Performance")
#     r2_score = model.score(X_test, y_test)
#     st.write(f"R² Score: {r2_score:.4f}")
# 
#     # Prediction input
#     st.subheader("Predict Housing Price")
#     user_input = {}
#     for col in X_filtered_unscaled.columns: # Ask for input for only the features used by the model
#         user_input[col] = st.number_input(f"Enter value for {col}", value=float(X_filtered_unscaled[col].mean())) # Provide a default for better UX
# 
#     if st.button("Predict"):
#         user_df = pd.DataFrame([user_input])
#         # Ensure the user_df has columns in the same order as X_filtered_unscaled before scaling
#         user_df = user_df[X_filtered_unscaled.columns]
# 
#         # Scale user input using the scaler fitted for the model
#         user_scaled = pd.DataFrame(scaler_for_model.transform(user_df), columns=X_filtered_unscaled.columns)
#         prediction = model.predict(user_scaled)
#         st.success(f"Predicted Sale Price: {prediction[0]:.2f}")



"""# Task
Modify the dataset loading logic to check if the path returned by `kagglehub.dataset_download` is a directory or a zip file. If it's a directory, use it directly as the data folder. If it's a zip file, extract its contents to a new folder. Then, load the CSV file from the identified data folder, perform feature scaling, calculate VIF to remove multicollinear features, split the data into training and testing sets, train a linear regression model, and finally display the R-squared score and a residual plot to confirm the successful execution of the data analysis pipeline.

## Adjust Dataset Loading Logic

### Subtask:
Modify the code to check if the downloaded path is a directory or a zip file. If it's a directory, use it directly as the `extract_folder`. If it's a zip file, proceed with extraction to the `extract_folder`.

**Reasoning**:
The previous code failed because `kagglehub.dataset_download` returned a directory path which was then incorrectly treated as a zip file. This step modifies the dataset loading logic to correctly handle both directory and zip file outputs from `kagglehub.dataset_download` by checking if the downloaded path is a directory or a zip file before attempting extraction, and then ensuring subsequent steps use the correct data folder.

## Final Task

### Subtask:
Confirm that the error has been resolved and the data analysis pipeline has executed successfully, including displaying the R-squared score and the residual plot.

## Summary:

### Q&A
1.  **Has the error been resolved and the data analysis pipeline executed successfully?**
    Yes, the dataset loading logic was successfully modified to handle both directory and zip file outputs from `kagglehub.dataset_download`. The entire data analysis pipeline, including feature scaling, VIF calculation, model training, and evaluation, executed successfully without errors.
2.  **Were the R-squared score and the residual plot displayed?**
    Yes, the model achieved an R-squared score of 0.8225 on the test set, and a residual plot was successfully generated and displayed.

### Data Analysis Key Findings
*   The dataset loading logic was successfully updated to dynamically handle both directory and zip file outputs from `kagglehub.dataset_download`, improving the pipeline's robustness.
*   The system correctly identified the downloaded dataset as a directory (`/kaggle/input/transformed-housing-data`) and subsequently located and loaded the `Transformed_Housing_Data2.csv` file.
*   After feature scaling and removing multicollinear features based on VIF scores, a linear regression model was trained on the processed data.
*   The trained linear regression model achieved a strong R-squared score of 0.8225 on the test set, indicating a good fit for predicting sale prices.
*   A residual plot was successfully generated, visually confirming the model's performance and the distribution of errors.

### Insights or Next Steps
*   The implemented flexible data loading mechanism ensures that the analysis pipeline can adapt to varying dataset download formats (directory or zip files) without manual intervention.
*   With an R-squared score of 0.8225, the current linear regression model provides a solid baseline for predicting housing sale prices; exploring advanced regression techniques or incorporating more features could potentially yield further improvements.
"""